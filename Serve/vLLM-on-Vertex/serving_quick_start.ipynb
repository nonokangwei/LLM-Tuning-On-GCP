{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47bae6f8-0d34-40a1-a577-1b9e0e55ca06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform in /opt/conda/lib/python3.10/site-packages (1.31.0)\n",
      "Collecting google-cloud-aiplatform\n",
      "  Obtaining dependency information for google-cloud-aiplatform from https://files.pythonhosted.org/packages/f9/00/13c8a1c052d8205875c888281a34ea10a181d5ca7b6d2003c28fb1da1a03/google_cloud_aiplatform-1.36.4-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_cloud_aiplatform-1.36.4-py2.py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.34.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.22.3)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (3.20.3)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (23.1)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.10.0)\n",
      "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (3.11.4)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.10.3)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.8.5.post1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.60.0)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.22.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.31.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.57.0)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.48.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.3.3)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.5.0)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.12.6)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (4.9)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.16.0)\n",
      "Requirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.26.16)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2023.7.22)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.5.0)\n",
      "Downloading google_cloud_aiplatform-1.36.4-py2.py3-none-any.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: google-cloud-aiplatform\n",
      "  Attempting uninstall: google-cloud-aiplatform\n",
      "    Found existing installation: google-cloud-aiplatform 1.31.0\n",
      "    Uninstalling google-cloud-aiplatform-1.31.0:\n",
      "      Successfully uninstalled google-cloud-aiplatform-1.31.0\n",
      "Successfully installed google-cloud-aiplatform-1.36.4\n"
     ]
    }
   ],
   "source": [
    "! pip3 install --upgrade google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a4f79fd-7668-44b5-a094-1a6909640a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from google.cloud import aiplatform, language, storage, aiplatform_v1\n",
    "from google.auth.transport.requests import Request\n",
    "import google.auth\n",
    "from google.api_core import operations_v1\n",
    "from google.longrunning import operations_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3aead9f0-58b7-498d-8ffe-2f3f579dcf6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "# Cloud project id.\n",
    "PROJECT_ID = \"project-kangwe-poc\"  # @param {type:\"string\"}\n",
    "\n",
    "# Region for launching jobs.\n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "# Cloud Storage bucket for storing experiments output.\n",
    "# Start with gs:// prefix, e.g. gs://foo_bucket.\n",
    "BUCKET_URI = \"gs://llama2ft-project-kangwe-poc-unique\"  # @param {type:\"string\"}\n",
    "\n",
    "! gcloud config set project $PROJECT_ID\n",
    "! gcloud services enable language.googleapis.com\n",
    "\n",
    "\n",
    "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
    "EXPERIMENT_BUCKET = os.path.join(BUCKET_URI, \"peft\")\n",
    "DATA_BUCKET = os.path.join(EXPERIMENT_BUCKET, \"data\")\n",
    "BASE_MODEL_BUCKET = os.path.join(EXPERIMENT_BUCKET, \"base_model\")\n",
    "MODEL_BUCKET = os.path.join(EXPERIMENT_BUCKET, \"model\")\n",
    "PREDICTION_BUCKET = os.path.join(EXPERIMENT_BUCKET, \"prediction\")\n",
    "\n",
    "# The service account looks like:\n",
    "# '@.iam.gserviceaccount.com'\n",
    "# Please go to https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console\n",
    "# and create service account with `Vertex AI User` and `Storage Object Admin` roles.\n",
    "# The service account for deploying fine tuned model.\n",
    "SERVICE_ACCOUNT = \"llamafinetune@project-kangwe-poc.iam.gserviceaccount.com\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf993168-01e1-4088-a154-3571a401b937",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "97f31c5d-cf7f-456a-b398-5b98b65e344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pre-built training, serving and evaluation docker images.\n",
    "VLLM_DOCKER_URI = \"us-central1-docker.pkg.dev/project-kangwe-poc/llmserve/vllm-runtime:1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "07713f28-2beb-478f-bd99-0c2d969aaae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://llama2ft-project-kangwe-poc-unique/llama2-7b-hf\n"
     ]
    }
   ],
   "source": [
    "base_model_name = \"llama2-7b-hf\"  # @param [\"llama2-7b-hf\", \"llama2-7b-chat-hf\", \"llama2-13b-hf\", \"llama2-13b-chat-hf\", \"llama2-70b-hf\", \"llama2-70b-chat-hf\"]\n",
    "base_model_id = os.path.join(BUCKET_URI, base_model_name)\n",
    "print(base_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0dd809dd-ae51-4965-bfe4-f0be53053ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_name_with_datetime(prefix: str) -> str:\n",
    "    \"\"\"Gets the job name with date time when triggering training or deployment\n",
    "    jobs in Vertex AI.\n",
    "    \"\"\"\n",
    "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
    "\n",
    "def upload_model_vllm(\n",
    "    model_name: str,\n",
    ") -> aiplatform.Model:\n",
    "    # start upload model\n",
    "    model_endpoint_url = f\"https://{REGION}-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{REGION}/models:upload\"\n",
    "\n",
    "    model_container_spec = {\n",
    "      \"imageUri\": VLLM_DOCKER_URI,\n",
    "      \"command\": [\n",
    "          \"python3\",\n",
    "          \"/root/scripts/launcher.py\"\n",
    "        ],\n",
    "      \"args\": [\n",
    "          \"--host=0.0.0.0\",\n",
    "          \"--port=7080\",\n",
    "          f\"--model={base_model_id}\",\n",
    "          \"--tensor-parallel-size=2\",\n",
    "          \"--swap-space=16\"\n",
    "      ],\n",
    "      \"ports\": [\n",
    "        {\n",
    "          \"containerPort\": 7080\n",
    "        }\n",
    "      ],\n",
    "      \"predictRoute\": \"/generate\",\n",
    "      \"healthRoute\": \"/ping\",\n",
    "      \"sharedMemorySizeMb\": \"6000\"\n",
    "    }\n",
    "\n",
    "    model_source_info = {\n",
    "        \"sourceType\": \"CUSTOM\"\n",
    "    }\n",
    "\n",
    "    model_infor = {\n",
    "      \"displayName\": f\"{model_name}\",\n",
    "      \"containerSpec\": model_container_spec,\n",
    "      \"modelSourceInfo\": model_source_info\n",
    "    }\n",
    "\n",
    "    model_request = {\n",
    "      \"modelId\": f\"{model_name}\",\n",
    "      \"model\": model_infor\n",
    "    }\n",
    "\n",
    "    # Get the default credentials\n",
    "    credentials, project = google.auth.default()\n",
    "\n",
    "    # Request an access token\n",
    "    credentials.refresh(Request())\n",
    "\n",
    "    # Get the access token\n",
    "    access_token = credentials.token\n",
    "\n",
    "    # Init Model upload request header\n",
    "    headers = {\n",
    "            \"Authorization\": f\"Bearer {access_token}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "\n",
    "    response = requests.post(model_endpoint_url, headers=headers, data=json.dumps(model_request))\n",
    "    \n",
    "    # parse the JSON response\n",
    "    data = response.json()\n",
    "\n",
    "    # extract the operation ID\n",
    "    operation_name = data['name']\n",
    "    \n",
    "    request = google.auth.transport.requests.Request()\n",
    "    channel = google.auth.transport.grpc.secure_authorized_channel(\n",
    "            credentials, request, f\"{REGION}-aiplatform.googleapis.com\")\n",
    "    \n",
    "    # create an operations client\n",
    "    client = operations_v1.OperationsClient(channel=channel)\n",
    "\n",
    "    # check if the operation is done\n",
    "    while(True):\n",
    "        # wait for 30 secs\n",
    "        time.sleep(30)\n",
    "        \n",
    "        # get the operation\n",
    "        operation = client.get_operation(operation_name)\n",
    "        \n",
    "        if operation.done:\n",
    "            if operation.HasField('response'):\n",
    "                print('Operation completed successfully')\n",
    "                # you can access the response via operation.response\n",
    "                break\n",
    "            elif operation.HasField('error'):\n",
    "                print('Operation failed')\n",
    "                # you can access the error message via operation.error.message\n",
    "                raise Exception(f\"This is error when upload model: {operation.error.message}\")\n",
    "        else:\n",
    "            print('Operation still in progress')\n",
    "    \n",
    "    # Define your model name\n",
    "    model_name = f\"projects/{PROJECT_ID}/locations/{REGION}/models/{model_name}\"\n",
    "            \n",
    "    return aiplatform.Model(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c5236b28-cd2b-4d01-b39c-ac441204f930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_model_vllm(\n",
    "    model_name: str,\n",
    "    model_id: str,\n",
    "    service_account: str,\n",
    "    machine_type: str = \"n1-standard-8\",\n",
    "    accelerator_type: str = \"NVIDIA_TESLA_V100\",\n",
    "    accelerator_count: int = 1,\n",
    ") -> tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    \"\"\"Deploys trained models with vLLM into Vertex AI.\"\"\"\n",
    "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
    "\n",
    "    vllm_args = [\n",
    "        \"--host=0.0.0.0\",\n",
    "        \"--port=7080\",\n",
    "        f\"--model={model_id}\",\n",
    "        f\"--tensor-parallel-size={accelerator_count}\",\n",
    "        \"--swap-space=16\",\n",
    "        # \"--gpu-memory-utilization=0.9\",\n",
    "        # \"--max_num_batched_tokens=4096\",\n",
    "        # \"--disable-log-stats\",\n",
    "    ]\n",
    "    \n",
    "#     vllm_args = [\n",
    "#         f\"--tensor_parallel_size={accelerator_count}\",\n",
    "#         f\"--model_gcs_uri={model_id}\"\n",
    "#     ]\n",
    "    \n",
    "    # model = aiplatform.Model.upload(\n",
    "    #     display_name=model_name,\n",
    "    #     serving_container_image_uri=VLLM_DOCKER_URI,\n",
    "    #     # serving_container_command=[\"python3\", \"/root/scripts/launcher.py\"],\n",
    "    #     serving_container_command=[\"python\", \"-m\", \"vllm.entrypoints.api_server\"],\n",
    "    #     serving_container_args=vllm_args,\n",
    "    #     serving_container_ports=[7080],\n",
    "    #     serving_container_predict_route=\"/generate\",\n",
    "    #     serving_container_health_route=\"/ping\",\n",
    "    # )\n",
    "     \n",
    "    model = upload_model_vllm(model_name=model_name)\n",
    "\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "    )\n",
    "    return model, endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0f72d6-34f7-45db-9b1d-819a352a73c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/725014442001/locations/us-central1/endpoints/6051133656162893824/operations/7675775816935931904\n",
      "Endpoint created. Resource name: projects/725014442001/locations/us-central1/endpoints/6051133656162893824\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/725014442001/locations/us-central1/endpoints/6051133656162893824')\n",
      "Operation still in progress\n",
      "Operation still in progress\n",
      "Operation still in progress\n",
      "Operation completed successfully\n",
      "Deploying model to Endpoint : projects/725014442001/locations/us-central1/endpoints/6051133656162893824\n",
      "Deploy Endpoint model backing LRO: projects/725014442001/locations/us-central1/endpoints/6051133656162893824/operations/8682611808629948416\n"
     ]
    }
   ],
   "source": [
    "# Finds Vertex AI prediction supported accelerators and regions in\n",
    "# https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
    "\n",
    "# Sets 1 V100 (16G) to deploy LLaMA2 7B models.\n",
    "# V100 serving has better throughput and latency performance than L4 serving.\n",
    "# machine_type = \"n1-standard-8\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_V100\"\n",
    "# accelerator_count = 1\n",
    "\n",
    "# Sets 1 L4 (24G) to deploy LLaMA2 7B models.\n",
    "# L4 serving is more cost efficient than V100 serving.\n",
    "# machine_type = \"g2-standard-8\"\n",
    "# accelerator_type = \"NVIDIA_L4\"\n",
    "# accelerator_count = 1\n",
    "\n",
    "# If A100 is not available, you may serve LLaMA2 13B models with multiple V100s\n",
    "# or L4s. Please keep in mind that the efficiency of serving with multiple\n",
    "# V100s or L4s is inferior to serving with 1 A100.\n",
    "# Sets 2 V100 (16G) to deploy LLaMA2 13B models.\n",
    "# V100 serving has better throughput and latency performance than L4 serving.\n",
    "# machine_type = \"n1-standard-16\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_V100\"\n",
    "# accelerator_count = 2\n",
    "\n",
    "# Sets 2 L4 (24G) to deploy LLaMA2 13B models.\n",
    "# L4 serving is more cost efficient than V100 serving.\n",
    "machine_type = \"g2-standard-24\"\n",
    "accelerator_type = \"NVIDIA_L4\"\n",
    "accelerator_count = 2\n",
    "\n",
    "# Sets A100 (40G) to deploy LLaMA2 13B models.\n",
    "# machine_type = \"a2-highgpu-1g\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_A100\"\n",
    "# accelerator_count = 1\n",
    "\n",
    "# Sets 8 L4 (24G) to deploy LLaMA2 70B models.\n",
    "# If you do not have access to 4 A100 (40G) GPUs, you may serve LLaMA 2 70B\n",
    "# models with 8 L4 (24G) GPUs.\n",
    "# machine_type = \"g2-standard-96\"\n",
    "# accelerator_type = \"NVIDIA_L4\"\n",
    "# accelerator_count = 8\n",
    "\n",
    "# Sets 4 A100 (40G) to deploy LLaMA2 70B models.\n",
    "# machine_type = \"a2-highgpu-4g\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_A100\"\n",
    "# accelerator_count = 4\n",
    "\n",
    "model_without_peft_vllm, endpoint_without_peft_vllm = deploy_model_vllm(\n",
    "    model_name=get_job_name_with_datetime(prefix=\"llama2-serve-vllm\"),\n",
    "    model_id=base_model_id,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c505d43a-dd96-4e5b-b9fc-4766416010a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6f4cf3-75a1-48f1-94f4-a68f47f6107a",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = {\n",
    "    \"prompt\": \"Hi, Google.\",\n",
    "    \"n\": 1,\n",
    "    \"max_tokens\": 50,\n",
    "    \"temperature\": 1.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"top_k\": 10,\n",
    "}\n",
    "response = endpoint_without_peft_vllm.predict(instances=[instance])\n",
    "print(response.predictions[0])"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m111"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db2b959-d545-44bd-a326-9903ed82bbdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Copyright 2023 Google LLC\n",
    "# \n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "# \n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "# \n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c90f7eaa",
   "metadata": {},
   "source": [
    "# Deepspeed Chat on Vertex AI\n",
    "\n",
    "\n",
    "NOTE: This is an example to test multi-mode training with DeepSpeed on Vertex. \n",
    "\n",
    "DeepspeedChat has 3 steps: SFT, Reward Model, and RLHF. We are only calling the SFT step here."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22bca191-3864-471c-becc-bb072adb09c5",
   "metadata": {},
   "source": [
    "## Setup Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69e20825-7406-4fef-9654-7975f6ca44fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artifact Registry Repo\n",
    "AR_REPO=\"llama2\"\n",
    "IMG_NAME=\"deepspeed-chat\"\n",
    "TAG=\"vertex\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da293c02-7be8-4559-a2df-10ee42b98192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project information\n",
    "PROJECT_ID=\"PROJECT_ID\"\n",
    "LOCATION=\"us-central1\"\n",
    "BUCKET=\"gs://BUCKET_NAME\"\n",
    "IMAGE_URI=f\"{LOCATION}-docker.pkg.dev/{PROJECT_ID}/{AR_REPO}/{IMG_NAME}:{TAG}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61c4f5dc-a733-462d-897c-24d17ffa914c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples/deepspeed-chat/deepspeed-chat.Dockerfile\n",
      "Sending build context to Docker daemon  4.584MB\n",
      "Step 1/31 : FROM us-docker.pkg.dev/vertex-ai/training/pytorch-gpu.1-13.py310:latest\n",
      " ---> 78c144ecd81c\n",
      "Step 2/31 : ENV PYTHONPATH=/opt/conda/lib/python3.10/site-packages:${PYTHONPATH}\n",
      " ---> Using cache\n",
      " ---> 5cd78fb02482\n",
      "Step 3/31 : RUN chmod 666 /var/log-storage/output.log\n",
      " ---> Using cache\n",
      " ---> a19250deeb80\n",
      "Step 4/31 : ENV STAGE_DIR=/tmp\n",
      " ---> Using cache\n",
      " ---> 626b4bc63513\n",
      "Step 5/31 : RUN mkdir -p ${STAGE_DIR}\n",
      " ---> Using cache\n",
      " ---> 9d1aef5c48c3\n",
      "Step 6/31 : RUN apt-get update  && apt-get install -y --no-install-recommends  openssh-client openssh-server  dnsutils iputils-ping  net-tools  libaio-dev cmake ninja-build pdsh  && rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> 84d3bb091c45\n",
      "Step 7/31 : ENV SSH_PORT=2222\n",
      " ---> Using cache\n",
      " ---> a10ccb585bb3\n",
      "Step 8/31 : COPY config/sshd_config.sed /tmp\n",
      " ---> Using cache\n",
      " ---> 70245923db33\n",
      "Step 9/31 : RUN sed -i -E -f /tmp/sshd_config.sed /etc/ssh/sshd_config\n",
      " ---> Using cache\n",
      " ---> c1fa93f450cd\n",
      "Step 10/31 : RUN sed -E -i 's/^(PATH=.*)/#\\1/' /etc/environment\n",
      " ---> Using cache\n",
      " ---> 6d92986774f6\n",
      "Step 11/31 : EXPOSE ${SSH_PORT}\n",
      " ---> Using cache\n",
      " ---> 94cc2f50eb92\n",
      "Step 12/31 : RUN useradd -ms /bin/bash vertex\n",
      " ---> Using cache\n",
      " ---> 37f914e90627\n",
      "Step 13/31 : RUN adduser vertex sudo\n",
      " ---> Using cache\n",
      " ---> 72f96405d570\n",
      "Step 14/31 : RUN echo '%sudo ALL=(ALL) NOPASSWD:ALL' >> /etc/sudoers\n",
      " ---> Using cache\n",
      " ---> 3ae586901bf3\n",
      "Step 15/31 : WORKDIR /home/vertex\n",
      " ---> Using cache\n",
      " ---> d0692d19c70a\n",
      "Step 16/31 : USER vertex\n",
      " ---> Using cache\n",
      " ---> 6dc4ef115cdd\n",
      "Step 17/31 : RUN mkdir -m 700 -p .ssh\n",
      " ---> Using cache\n",
      " ---> ef45481e7c34\n",
      "Step 18/31 : COPY config/requirements.txt requirements.txt\n",
      " ---> Using cache\n",
      " ---> d09b6dbc472d\n",
      "Step 19/31 : RUN pip install -r requirements.txt\n",
      " ---> Using cache\n",
      " ---> 9953fd5d79d0\n",
      "Step 20/31 : RUN DS_BUILD_CPU_ADAM=1 DS_BUILD_FUSED_ADAM=1     DS_BUILD_FUSED_LAMB=1 DS_BUILD_TRANSFORMER=1     DS_BUILD_TRANSFORMER_INFERENCE=1 DS_BUILD_STOCHASTIC_TRANSFORMER=1     DS_BUILD_UTILS=1 DS_BUILD_SPARSE_ATTN=0     pip install \"deepspeed==0.10.2\" --global-option=\"build_ext\"\n",
      " ---> Using cache\n",
      " ---> 8d207dbcadf7\n",
      "Step 21/31 : RUN pip install py7zr\n",
      " ---> Using cache\n",
      " ---> db953545e006\n",
      "Step 22/31 : COPY scripts/*.sh ./\n",
      " ---> Using cache\n",
      " ---> c74592dfaa82\n",
      "Step 23/31 : RUN mkdir -p .cache/huggingface\n",
      " ---> Using cache\n",
      " ---> 93ede75365a7\n",
      "Step 24/31 : COPY token .cache/huggingface/token\n",
      " ---> Using cache\n",
      " ---> 2c2414f7160f\n",
      "Step 25/31 : COPY third_party/deepspeed_examples/utils utils\n",
      " ---> Using cache\n",
      " ---> 159dea26e0c0\n",
      "Step 26/31 : COPY third_party/deepspeed_examples/main.py .\n",
      " ---> Using cache\n",
      " ---> 46278d2878f9\n",
      "Step 27/31 : COPY examples/deepspeed-chat/deepspeed_train.sh .\n",
      " ---> Using cache\n",
      " ---> 94d861277e4a\n",
      "Step 28/31 : COPY examples/deepspeed-chat/deepspeed_train_lora.sh .\n",
      " ---> 70485f5a5a0e\n",
      "Step 29/31 : COPY examples/deepspeed-chat/deepspeed_train_sft.sh .\n",
      " ---> 7a21f6a503b2\n",
      "Step 30/31 : COPY examples/deepspeed-chat/sleep.sh .\n",
      " ---> bc874c47f569\n",
      "Step 31/31 : ENTRYPOINT [\"bash\", \"./sleep.sh\"]\n",
      " ---> Running in 33e4715cb43b\n",
      "Removing intermediate container 33e4715cb43b\n",
      " ---> 377cf63010af\n",
      "Successfully built 377cf63010af\n",
      "Successfully tagged us-central1-docker.pkg.dev/argolis-lsj-test/llama2/deepspeed-chat:1020\n"
     ]
    }
   ],
   "source": [
    "# build dockerfile\n",
    "EXAMPLE_DIR=\"deepspeed-chat\"\n",
    "DOCKERFILE=f\"examples/{EXAMPLE_DIR}/{IMG_NAME}.Dockerfile\"\n",
    "!echo $DOCKERFILE\n",
    "!docker build . -t $IMAGE_URI -f $DOCKERFILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c18bb81-eab9-4851-9de0-aa35234f555b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this throws error\n",
    "# add \"us-docker.pkg.dev\": \"gcloud\" to /home/jupyter/.docker/config.json\n",
    "!gcloud auth configure-docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c12f804e-d987-40a3-87e6-f931bf0ab533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-central1-docker.pkg.dev/argolis-lsj-test/llama2/deepspeed-chat:1020\n",
      "The push refers to repository [us-central1-docker.pkg.dev/argolis-lsj-test/llama2/deepspeed-chat]\n",
      "\n",
      "\u001b[1Bb12a657e: Preparing \n",
      "\u001b[1Bdd9f5d1b: Preparing \n",
      "\u001b[1Bfb35c5a7: Preparing \n",
      "\u001b[1Bc835feb3: Preparing \n",
      "\u001b[1Becc297d5: Preparing \n",
      "\u001b[1B12f16e66: Preparing \n",
      "\u001b[1B9d07f065: Preparing \n",
      "\u001b[1Bee50daf9: Preparing \n",
      "\u001b[1B2839c42e: Preparing \n",
      "\u001b[1Bc1a7a815: Preparing \n",
      "\u001b[1B04c8cac9: Preparing \n",
      "\u001b[1B9b3321f2: Preparing \n",
      "\u001b[1B7f13fbf0: Preparing \n",
      "\u001b[1Bfe2a49dd: Preparing \n",
      "\u001b[1B64833029: Preparing \n",
      "\u001b[1B183c9ad8: Preparing \n",
      "\u001b[1B74c7df37: Preparing \n",
      "\u001b[1B09d25265: Preparing \n",
      "\u001b[1B225e8740: Preparing \n",
      "\u001b[1Bdc2fb310: Preparing \n",
      "\u001b[1Ba15e9e6d: Preparing \n",
      "\u001b[1Bb05b4a66: Preparing \n",
      "\u001b[1B03eb5103: Preparing \n",
      "\u001b[1B105d38de: Preparing \n",
      "\u001b[1B6867eca5: Preparing \n",
      "\u001b[1Beb8da3b6: Preparing \n",
      "\u001b[1B3741a401: Preparing \n",
      "\u001b[1B4314a1a9: Preparing \n",
      "\u001b[1Be1a4db2c: Preparing \n",
      "\u001b[1B2d93004e: Preparing \n",
      "\u001b[1Bc5d23056: Preparing \n",
      "\u001b[1B988466f1: Preparing \n",
      "\u001b[1Ba520fb4d: Preparing \n",
      "\u001b[1B1fee7951: Preparing \n",
      "\u001b[1Bede7a422: Preparing \n",
      "\u001b[1Bc6d83751: Preparing \n",
      "\u001b[24Be2a49dd: Waiting g \n",
      "\u001b[1B475b74ad: Preparing \n",
      "\u001b[25B4833029: Waiting g \n",
      "\u001b[22B25e8740: Waiting g \n",
      "\u001b[21B15e9e6d: Waiting g \n",
      "\u001b[1B19283e37: Preparing \n",
      "\u001b[22B05b4a66: Waiting g \n",
      "\u001b[27B9d25265: Waiting g \n",
      "\u001b[1B1dbe19ae: Preparing \n",
      "\u001b[40Bd07f065: Waiting g \n",
      "\u001b[1B48c7a814: Preparing \n",
      "\u001b[1Bfded9150: Preparing \n",
      "\u001b[18B88466f1: Waiting g \n",
      "\u001b[1B59e9243c: Preparing \n",
      "\u001b[22Bd93004e: Waiting g \n",
      "\u001b[42B4c8cac9: Waiting g \n",
      "\u001b[27B741a401: Waiting g \n",
      "\u001b[1B2cf1bac1: Preparing \n",
      "\u001b[16Bc30cd40: Waiting g \n",
      "\u001b[22Bde7a422: Waiting g \n",
      "\u001b[1Bbf18a086: Preparing \n",
      "\u001b[23B6d83751: Waiting g \n",
      "\u001b[17B2a15377: Waiting g \n",
      "\u001b[23B75b74ad: Waiting g \n",
      "\u001b[25B1826dae: Waiting g \n",
      "\u001b[8B979563cb: Waiting g \n",
      "\u001b[1B7090d6d6: Preparing \n",
      "\u001b[19Bcf42f7a: Waiting g \n",
      "\u001b[22B451103f: Waiting g \n",
      "\u001b[14Bb414017: Waiting g \n",
      "\u001b[14Bcf1bac1: Waiting g \n",
      "\u001b[17B09eecef: Waiting g \n",
      "\u001b[12B4255795: Waiting g \n",
      "\u001b[10B7453a10: Waiting g \n",
      "\u001b[3B183bef21: Layer already exists 3kB\u001b[68A\u001b[2K\u001b[64A\u001b[2K\u001b[62A\u001b[2K\u001b[60A\u001b[2K\u001b[69A\u001b[2K\u001b[51A\u001b[2K\u001b[44A\u001b[2K\u001b[38A\u001b[2K\u001b[28A\u001b[2K\u001b[21A\u001b[2K\u001b[15A\u001b[2K\u001b[5A\u001b[2K1020: digest: sha256:00422bb60c701b7294c04b2fa66aef8460988a567c83b29abad17795fcef80b5 size: 15361\n"
     ]
    }
   ],
   "source": [
    "# Make sure the repo specified in $AR_REPO exists.\n",
    "# Push the docker image\n",
    "!echo $IMAGE_URI\n",
    "!docker push $IMAGE_URI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e3583fc-de23-428d-92c7-82fb33153c35",
   "metadata": {},
   "source": [
    "## Test container with aiplatform.CustomJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94457cc4-ef09-470d-934d-a14189a95300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET, location=LOCATION)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a8cbd6c",
   "metadata": {},
   "source": [
    "### Configure the custom job\n",
    "\n",
    "\n",
    "\n",
    "- DATA_PATHS - dataset name in huggingface or GCS path\n",
    "- MODEL_PATH - model name in huggingface or GCS path\n",
    "- DATA_SPLIT - \"10,40,50\" means 10% of the data is used for SFT. The DeepspeedChat code converts the string into fractions (data_utils.py). We do not demo it in this notebook.\n",
    "- ZERO_STAGE - ZERO stage\n",
    "- PER_DEVICE_BATCH_SIZE - training batch size\n",
    "\n",
    "The demo shows how to fine-tune Llama2-7b-hf and samsum dataset using SFT(*train_deepspeed_sft.sh*) or LoRA (*train_deepspeed_lora.sh*). To be competable with samsum dataset and users' customized datasets, the samsum interface and customized dataset interface are implemented in *third_party/utils/data/data_utils.py* and *raw_datasets.py*.\n",
    "\n",
    "Model uploaded to Vertex AI Model Regitry is also implemented in *main.py* and *model_registry.py*. If *serving_container_image_uri* is not None, the registration process will start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13e7e084-a6b7-4795-901f-7b081814249c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "703099487153-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "# configure machine specs, and environment variables \n",
    "worker_pool_specs = [\n",
    "    # `WorkerPoolSpec` for worker pool 0, primary replica, required  \n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"g2-standard-96\",\n",
    "            \"accelerator_type\": \"NVIDIA_L4\",\n",
    "            \"accelerator_count\": 8,       \n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": IMAGE_URI,\n",
    "            \"command\": [],\n",
    "            \"args\": [],\n",
    "            \"env\": [\n",
    "                {\"name\": \"MODEL_PATH\", \"value\": \"/gcs/deepspeed_repo/base_model/Llama-2-7b-hf/Llama-2-7b-hf\"},                        \n",
    "                {\"name\": \"DATA_PATHS\", \"value\": \"/gcs/deepspeed_repo/dataset/samsum\"},\n",
    "                {\"name\": \"DATA_SPLIT\", \"value\": \"10,0,0\"},                      \n",
    "                {\"name\": \"ZERO_STAGE\", \"value\": \"3\"},\n",
    "                {\"name\": \"PER_DEVICE_BATCH_SIZE\", \"value\": \"4\"},\n",
    "            ],                \n",
    "        },\n",
    "        \"disk_spec\": {\n",
    "            \"boot_disk_size_gb\": 1000,            \n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # {\n",
    "    #    \"machine_spec\": {\n",
    "    #         \"machine_type\": \"g2-standard-48\",\n",
    "    #         \"accelerator_type\": \"NVIDIA_L4\",\n",
    "    #         \"accelerator_count\": 4,           \n",
    "    #    },\n",
    "    #    \"replica_count\": 1,        \n",
    "    #    \"container_spec\": {\n",
    "    #        \"image_uri\": IMAGE_URI,\n",
    "    #        \"command\": [],\n",
    "    #         \"args\": [],\n",
    "    #         \"env\": [\n",
    "    #             {\"name\": \"MODEL_PATH\", \"value\": \"/gcs/deepspeed_repo/base_model/Llama-2-7b-hf/Llama-2-7b-hf\"},                        \n",
    "    #             {\"name\": \"DATA_PATHS\", \"value\": \"samsum\"},\n",
    "    #             {\"name\": \"DATA_SPLIT\", \"value\": \"10,0,0\"},                        \n",
    "    #             {\"name\": \"ZERO_STAGE\", \"value\": \"3\"},\n",
    "    #             {\"name\": \"PER_DEVICE_BATCH_SIZE\", \"value\": \"4\"}, \n",
    "    #         ],\n",
    "    #    },        \n",
    "    #    \"disk_spec\": {\n",
    "    #         \"boot_disk_size_gb\": 1000,            \n",
    "    #    }        \n",
    "    # },\n",
    "]\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "JOB_NAME  = \"DeepSpeed Chat Test \" + TIMESTAMP\n",
    "\n",
    "my_job = aiplatform.CustomJob(\n",
    "    display_name=JOB_NAME,    \n",
    "    worker_pool_specs=worker_pool_specs,\n",
    ")\n",
    "\n",
    "# Checking Service account that will launch the job\n",
    "!gcloud config get account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "454bf780-ddc1-4795-ac37-2fcb7abedeaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects/703099487153/locations/us-central1/tensorboards/2069606338916253696\n"
     ]
    }
   ],
   "source": [
    "#####\n",
    "# Either create or reuse a tensorboard\n",
    "# tensorboard = aiplatform.Tensorboard.create(\n",
    "#    display_name=JOB_NAME,\n",
    "# )\n",
    "# \n",
    "tensorboard_name = \"VERTEX AI TENSORBOARD ID\"\n",
    "tensorboard = aiplatform.Tensorboard(tensorboard_name=tensorboard_name)\n",
    "# \n",
    "print(tensorboard.resource_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79f62ff5",
   "metadata": {},
   "source": [
    "### Running the CustomJob\n",
    "\n",
    "Custom Service Account - https://cloud.google.com/vertex-ai/docs/general/custom-service-account. For custom service account, be sure to first grant the SA running this notebook the \"Service Account User\" role, otherwise you won't be able to launch the job with the custom service account.\n",
    "\n",
    "Tensorboard - https://cloud.google.com/vertex-ai/docs/experiments/tensorboard-training. Your training script must be configured to write TensorBoard logs to the Cloud Storage bucket, the location of which the Vertex AI Training Service will automatically make available through a predefined environment variable AIP_TENSORBOARD_LOG_DIR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb6c5133-e0b0-427d-ae81-da5ea8ab5c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating CustomJob\n",
      "CustomJob created. Resource name: projects/703099487153/locations/us-central1/customJobs/2406458012601417728\n",
      "To use this CustomJob in another session:\n",
      "custom_job = aiplatform.CustomJob.get('projects/703099487153/locations/us-central1/customJobs/2406458012601417728')\n",
      "View Custom Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/2406458012601417728?project=703099487153\n",
      "View Tensorboard:\n",
      "https://us-central1.tensorboard.googleusercontent.com/experiment/projects+703099487153+locations+us-central1+tensorboards+2069606338916253696+experiments+2406458012601417728\n"
     ]
    }
   ],
   "source": [
    "\n",
    "my_job.submit(    \n",
    "    enable_web_access=True, # For debugging\n",
    "    service_account=\"SERVICE ACCOUNT NAME\",\n",
    "    tensorboard=tensorboard.resource_name,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35b18b15",
   "metadata": {},
   "source": [
    "While the Deepspeed Chat team has auto-tuning on roadmap, if you encounter CUDA OOM right now their advice is:\n",
    "- Reduce `--per_device_*_batch_size`,\n",
    "- Increase `--zero_stage {0,1,2,3}` on multi-gpu setups,\n",
    "- Enable `--gradient_checkpointing` or `--only_optimize_lora`,\n",
    "- Increase `--gradient_accumulate_steps {#}`, higher number reduces communication of gradients between steps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25e7dde8",
   "metadata": {},
   "source": [
    "# Configurations backup\n",
    "\n",
    "Testing facebook/opt-125m and Dahoas/synthetic-instruct-gptj-pairwise with 2 1xT4@n1-standard-4:\n",
    "\n",
    "PER_DEVICE_BATCH_SIZE - 8 will utilize < half a T4's memory on each of the 2 nodes, 32 uses the memory 70+%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eee199-e5df-4e5e-8761-aa99169cfcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt-125m backup\n",
    "{\"name\": \"MODEL_PATH\", \"value\": \"facebook/opt-125m\"},                        \n",
    "{\"name\": \"DATA_PATHS\", \"value\": \"Dahoas/synthetic-instruct-gptj-pairwise\"},                        \n",
    "{\"name\": \"DATA_SPLIT\", \"value\": \"10,40,50\"},\n",
    "{\"name\": \"ZERO_STAGE\", \"value\": \"3\"},\n",
    "{\"name\": \"PER_DEVICE_BATCH_SIZE\", \"value\": \"32\"},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575e9dbe-a59b-48ed-b284-42024c6adb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama2 backup\n",
    "# If download llama2 from huggingface, copy your huggingface read token in token file first.\n",
    "{\"name\": \"MODEL_PATH\", \"value\": \"meta-llama/Llama-2-7b-hf\"},                        \n",
    "{\"name\": \"DATA_PATHS\", \"value\": \"Dahoas/rm-static Dahoas/full-hh-rlhf Dahoas/synthetic-instruct-gptj-pairwise yitingxie/rlhf-reward-datasets\"},                        \n",
    "{\"name\": \"DATA_SPLIT\", \"value\": \"2,4,4\"},\n",
    "{\"name\": \"ZERO_STAGE\", \"value\": \"3\"},\n",
    "{\"name\": \"PER_DEVICE_BATCH_SIZE\", \"value\": \"4\"}, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11538d3-5e33-4aee-8f5f-4ca2a02ba4f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m110",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m110"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

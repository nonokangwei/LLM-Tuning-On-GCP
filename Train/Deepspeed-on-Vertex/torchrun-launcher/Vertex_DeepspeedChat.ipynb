{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db2b959-d545-44bd-a326-9903ed82bbdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Copyright 2023 Google LLC\n",
    "# \n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "# \n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "# \n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b5f1c63",
   "metadata": {},
   "source": [
    "# Deepspeed Chat on Vertex AI\n",
    "\n",
    "NOTE: This is an example to test multi-mode training with DeepSpeed on Vertex. This folder is to use torchrun launcher to launcher the multi-process, not use default deepspeed launcher.\n",
    "\n",
    "DeepspeedChat has 3 steps: SFT, Reward Model, and RLHF. We are only calling the SFT step here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bca191-3864-471c-becc-bb072adb09c5",
   "metadata": {},
   "source": [
    "# Setup Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e20825-7406-4fef-9654-7975f6ca44fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artifact Registry Repo\n",
    "AR_REPO=\"llama2\"\n",
    "IMG_NAME=\"deepspeed-chat\"\n",
    "TAG=\"vertex-torchrun\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da293c02-7be8-4559-a2df-10ee42b98192",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID=\"PROJECT_ID\"\n",
    "LOCATION=\"us-central1\"\n",
    "BUCKET=\"gs://BUCKET_NAME\"\n",
    "IMAGE_URI=f\"{LOCATION}-docker.pkg.dev/{PROJECT_ID}/{AR_REPO}/{IMG_NAME}:{TAG}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e771bc68-e61d-4cce-a7b5-119f3eecfd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/vertex-deepspeed\n"
     ]
    }
   ],
   "source": [
    "%cd ~/vertex-deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c4f5dc-a733-462d-897c-24d17ffa914c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples/deepspeed-chat/deepspeed-chat.Dockerfile\n",
      "Sending build context to Docker daemon  4.585MB\n",
      "Step 1/31 : FROM us-docker.pkg.dev/vertex-ai/training/pytorch-gpu.1-13.py310:latest\n",
      " ---> 78c144ecd81c\n",
      "Step 2/31 : ENV PYTHONPATH=/opt/conda/lib/python3.10/site-packages:${PYTHONPATH}\n",
      " ---> Using cache\n",
      " ---> 5cd78fb02482\n",
      "Step 3/31 : RUN chmod 666 /var/log-storage/output.log\n",
      " ---> Using cache\n",
      " ---> a19250deeb80\n",
      "Step 4/31 : ENV STAGE_DIR=/tmp\n",
      " ---> Using cache\n",
      " ---> 626b4bc63513\n",
      "Step 5/31 : RUN mkdir -p ${STAGE_DIR}\n",
      " ---> Using cache\n",
      " ---> 9d1aef5c48c3\n",
      "Step 6/31 : RUN apt-get update  && apt-get install -y --no-install-recommends  openssh-client openssh-server  dnsutils iputils-ping  net-tools  libaio-dev cmake ninja-build pdsh  && rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> 84d3bb091c45\n",
      "Step 7/31 : ENV SSH_PORT=2222\n",
      " ---> Using cache\n",
      " ---> a10ccb585bb3\n",
      "Step 8/31 : COPY config/sshd_config.sed /tmp\n",
      " ---> Using cache\n",
      " ---> 70245923db33\n",
      "Step 9/31 : RUN sed -i -E -f /tmp/sshd_config.sed /etc/ssh/sshd_config\n",
      " ---> Using cache\n",
      " ---> c1fa93f450cd\n",
      "Step 10/31 : RUN sed -E -i 's/^(PATH=.*)/#\\1/' /etc/environment\n",
      " ---> Using cache\n",
      " ---> 6d92986774f6\n",
      "Step 11/31 : EXPOSE ${SSH_PORT}\n",
      " ---> Using cache\n",
      " ---> 94cc2f50eb92\n",
      "Step 12/31 : RUN useradd -ms /bin/bash vertex\n",
      " ---> Using cache\n",
      " ---> 37f914e90627\n",
      "Step 13/31 : RUN adduser vertex sudo\n",
      " ---> Using cache\n",
      " ---> 72f96405d570\n",
      "Step 14/31 : RUN echo '%sudo ALL=(ALL) NOPASSWD:ALL' >> /etc/sudoers\n",
      " ---> Using cache\n",
      " ---> 3ae586901bf3\n",
      "Step 15/31 : WORKDIR /home/vertex\n",
      " ---> Using cache\n",
      " ---> d0692d19c70a\n",
      "Step 16/31 : USER vertex\n",
      " ---> Using cache\n",
      " ---> 6dc4ef115cdd\n",
      "Step 17/31 : RUN mkdir -m 700 -p .ssh\n",
      " ---> Using cache\n",
      " ---> ef45481e7c34\n",
      "Step 18/31 : COPY config/requirements.txt requirements.txt\n",
      " ---> Using cache\n",
      " ---> d09b6dbc472d\n",
      "Step 19/31 : RUN pip install -r requirements.txt\n",
      " ---> Using cache\n",
      " ---> 9953fd5d79d0\n",
      "Step 20/31 : RUN DS_BUILD_CPU_ADAM=1 DS_BUILD_FUSED_ADAM=1     DS_BUILD_FUSED_LAMB=1 DS_BUILD_TRANSFORMER=1     DS_BUILD_TRANSFORMER_INFERENCE=1 DS_BUILD_STOCHASTIC_TRANSFORMER=1     DS_BUILD_UTILS=1 DS_BUILD_SPARSE_ATTN=0     pip install \"deepspeed==0.10.2\" --global-option=\"build_ext\"\n",
      " ---> Using cache\n",
      " ---> 8d207dbcadf7\n",
      "Step 21/31 : RUN pip install py7zr\n",
      " ---> Using cache\n",
      " ---> db953545e006\n",
      "Step 22/31 : COPY scripts/*.sh ./\n",
      " ---> a5e2401bee09\n",
      "Step 23/31 : RUN mkdir -p .cache/huggingface\n",
      " ---> Running in 4feb02ecb28a\n",
      "Removing intermediate container 4feb02ecb28a\n",
      " ---> 0d5a3eadc5c7\n",
      "Step 24/31 : COPY token .cache/huggingface/token\n",
      " ---> 487a96c88784\n",
      "Step 25/31 : COPY third_party/deepspeed_examples/utils utils\n",
      " ---> 8f67ad68e54f\n",
      "Step 26/31 : COPY third_party/deepspeed_examples/main.py .\n",
      " ---> f5895a121593\n",
      "Step 27/31 : COPY examples/deepspeed-chat/deepspeed_train.sh .\n",
      " ---> 6a496450602d\n",
      "Step 28/31 : COPY examples/deepspeed-chat/deepspeed_train_lora.sh .\n",
      " ---> 6b8852d6147b\n",
      "Step 29/31 : COPY examples/deepspeed-chat/deepspeed_train_sft.sh .\n",
      " ---> e5a415fe3c34\n",
      "Step 30/31 : COPY examples/deepspeed-chat/sleep.sh .\n",
      " ---> 596df8e5ca4c\n",
      "Step 31/31 : ENTRYPOINT [\"bash\", \"./start.sh\", \"deepspeed_train_sft.sh\"]\n",
      " ---> Running in 9c59bcf7723e\n",
      "Removing intermediate container 9c59bcf7723e\n",
      " ---> d2efdfe6968e\n",
      "Successfully built d2efdfe6968e\n",
      "Successfully tagged us-central1-docker.pkg.dev/argolis-lsj-test/llama2/deepspeed-chat:1023\n"
     ]
    }
   ],
   "source": [
    "# build docker image\n",
    "EXAMPLE_DIR=\"deepspeed-chat\"\n",
    "DOCKERFILE=f\"examples/{EXAMPLE_DIR}/{IMG_NAME}.Dockerfile\"\n",
    "!echo $DOCKERFILE\n",
    "!docker build . -t $IMAGE_URI -f $DOCKERFILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c18bb81-eab9-4851-9de0-aa35234f555b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this throws error\n",
    "# add \"us-docker.pkg.dev\": \"gcloud\" to /home/jupyter/.docker/config.json\n",
    "!gcloud auth configure-docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12f804e-d987-40a3-87e6-f931bf0ab533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-central1-docker.pkg.dev/argolis-lsj-test/llama2/deepspeed-chat:1023\n",
      "The push refers to repository [us-central1-docker.pkg.dev/argolis-lsj-test/llama2/deepspeed-chat]\n",
      "\n",
      "\u001b[1Bf4415b63: Preparing \n",
      "\u001b[1Ba7522e63: Preparing \n",
      "\u001b[1B8c4c553f: Preparing \n",
      "\u001b[1B88d8942e: Preparing \n",
      "\u001b[1Bc2bfada6: Preparing \n",
      "\u001b[1B73166935: Preparing \n",
      "\u001b[1Bb29cfdc5: Preparing \n",
      "\u001b[1B6064ce0d: Preparing \n",
      "\u001b[1Bbb27861f: Preparing \n",
      "\u001b[1Bc1a7a815: Preparing \n",
      "\u001b[1B04c8cac9: Preparing \n",
      "\u001b[1B9b3321f2: Preparing \n",
      "\u001b[1B7f13fbf0: Preparing \n",
      "\u001b[1Bfe2a49dd: Preparing \n",
      "\u001b[1B64833029: Preparing \n",
      "\u001b[1B183c9ad8: Preparing \n",
      "\u001b[1B74c7df37: Preparing \n",
      "\u001b[1B09d25265: Preparing \n",
      "\u001b[1B225e8740: Preparing \n",
      "\u001b[1Bdc2fb310: Preparing \n",
      "\u001b[1Ba15e9e6d: Preparing \n",
      "\u001b[1Bb05b4a66: Preparing \n",
      "\u001b[1B03eb5103: Preparing \n",
      "\u001b[1B105d38de: Preparing \n",
      "\u001b[19B29cfdc5: Waiting g \n",
      "\u001b[1Beb8da3b6: Preparing \n",
      "\u001b[1B3741a401: Preparing \n",
      "\u001b[1B4314a1a9: Preparing \n",
      "\u001b[1Be1a4db2c: Preparing \n",
      "\u001b[1B2d93004e: Preparing \n",
      "\u001b[1Bc5d23056: Preparing \n",
      "\u001b[1B988466f1: Preparing \n",
      "\u001b[1Ba520fb4d: Preparing \n",
      "\u001b[26Bb27861f: Waiting g \n",
      "\u001b[1Bede7a422: Preparing \n",
      "\u001b[26B4c8cac9: Waiting g \n",
      "\u001b[1Bc1826dae: Preparing \n",
      "\u001b[1B475b74ad: Preparing \n",
      "\u001b[1B8569388a: Preparing \n",
      "\u001b[1Bcc30cd40: Preparing \n",
      "\u001b[1B47d1078a: Preparing \n",
      "\u001b[1B19283e37: Preparing \n",
      "\u001b[1Bb2a15377: Preparing \n",
      "\u001b[1Be451103f: Preparing \n",
      "\u001b[1B1dbe19ae: Preparing \n",
      "\u001b[1Bacf42f7a: Preparing \n",
      "\u001b[1B48c7a814: Preparing \n",
      "\u001b[1Bfded9150: Preparing \n",
      "\u001b[1B166a99a1: Preparing \n",
      "\u001b[1B59e9243c: Preparing \n",
      "\u001b[1B3cbe0f06: Preparing \n",
      "\u001b[1B809eecef: Preparing \n",
      "\u001b[1B4b414017: Preparing \n",
      "\u001b[1B2cf1bac1: Preparing \n",
      "\u001b[1B979563cb: Preparing \n",
      "\u001b[1B0a62515d: Preparing \n",
      "\u001b[1Bbf18a086: Preparing \n",
      "\u001b[1Bd4255795: Preparing \n",
      "\u001b[1B4e925c07: Preparing \n",
      "\u001b[1Bad0ce3ec: Preparing \n",
      "\u001b[1B17453a10: Preparing \n",
      "\u001b[1Be1a1cbfb: Preparing \n",
      "\u001b[1B7090d6d6: Preparing \n",
      "\u001b[1B1fa7f0e3: Preparing \n",
      "\u001b[1Bef4e19ae: Preparing \n",
      "\u001b[1Bd9356ee8: Preparing \n",
      "\u001b[1B66373eb9: Preparing \n",
      "\u001b[1B7a859620: Preparing \n",
      "\u001b[1B183bef21: Preparing \n",
      "\u001b[1B69562af2: Preparing \n",
      "\u001b[1Bd8cea54a: Layer already exists 3kB\u001b[69A\u001b[2K\u001b[70A\u001b[2K\u001b[66A\u001b[2K\u001b[63A\u001b[2K\u001b[61A\u001b[2K\u001b[60A\u001b[2K\u001b[59A\u001b[2K\u001b[57A\u001b[2K\u001b[53A\u001b[2K\u001b[50A\u001b[2K\u001b[44A\u001b[2K\u001b[38A\u001b[2K\u001b[31A\u001b[2K\u001b[26A\u001b[2K\u001b[18A\u001b[2K\u001b[12A\u001b[2K\u001b[6A\u001b[2K1023: digest: sha256:bf904b566bfa283c252e664e99b6e47e7d52e4d84f2ab203d515b77a1b270dad size: 15361\n"
     ]
    }
   ],
   "source": [
    "# Make sure the repo specified in $AR_REPO exists.\n",
    "# push the image\n",
    "!echo $IMAGE_URI\n",
    "!docker push $IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3583fc-de23-428d-92c7-82fb33153c35",
   "metadata": {},
   "source": [
    "# Test container with aiplatform.CustomJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94457cc4-ef09-470d-934d-a14189a95300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET, location=LOCATION)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1fa0bb59",
   "metadata": {},
   "source": [
    "### Configure the custom job\n",
    "\n",
    "\n",
    "- DATA_PATHS - dataset name in huggingface or GCS path\n",
    "- MODEL_PATH - model name in huggingface or GCS path\n",
    "- DATA_SPLIT - \"10,40,50\" means 10% of the data is used for SFT. The DeepspeedChat code converts the string into fractions (data_utils.py). We do not demo it in this notebook.\n",
    "- ZERO_STAGE - ZERO stage\n",
    "- PER_DEVICE_BATCH_SIZE - training batch size\n",
    "\n",
    "The demo shows how to fine-tune Llama2-7b-hf and samsum dataset using SFT(*train_deepspeed_sft.sh*) or LoRA (*train_deepspeed_lora.sh*). To be competable with samsum dataset and users' customized datasets, the samsum interface and customized dataset interface are implemented in *third_party/utils/data/data_utils.py* and *raw_datasets.py*.\n",
    "\n",
    "Model uploaded to Vertex AI Model Regitry is also implemented in *main.py* and *model_registry.py*. If *serving_container_image_uri* is not None, the registration process will start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e7e084-a6b7-4795-901f-7b081814249c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "703099487153-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "# configure machine specs, and environment variables \n",
    "TOTAL_GPU_NEEDED = 8\n",
    "CPU_UNIT = 12\n",
    "ACCELERATOR_COUNT = 4 #per node\n",
    "CPU_COUNT = CPU_UNIT * ACCELERATOR_COUNT\n",
    "REPLICA_COUNT = TOTAL_GPU_NEEDED / ACCELERATOR_COUNT\n",
    "\n",
    "ENVS = [\n",
    "    {\"name\": \"MODEL_PATH\", \"value\": \"/gcs/deepspeed_repo/base_model/Llama-2-7b-hf/Llama-2-7b-hf\"},                        \n",
    "    {\"name\": \"DATA_PATHS\", \"value\": \"/gcs/deepspeed_repo/dataset/samsum\"},\n",
    "    {\"name\": \"DATA_SPLIT\", \"value\": \"10,0,0\"},                       \n",
    "    {\"name\": \"ZERO_STAGE\", \"value\": \"3\"},\n",
    "    {\"name\": \"PER_DEVICE_BATCH_SIZE\", \"value\": \"4\"},\n",
    "    {\"name\": \"NUM_GPU_PER_NODE\", \"value\": f\"{ACCELERATOR_COUNT}\"},\n",
    "                \n",
    "]\n",
    "# `WorkerPoolSpec` for worker pool 0, primary replica, required  \n",
    "worker_pool_specs_1 = {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": f\"g2-standard-{CPU_COUNT}\",\n",
    "            \"accelerator_type\": \"NVIDIA_L4\",\n",
    "            \"accelerator_count\": ACCELERATOR_COUNT,       \n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": IMAGE_URI,\n",
    "            \"command\": [],\n",
    "            \"args\": [],\n",
    "            \"env\": ENVS,                \n",
    "        },\n",
    "        \"disk_spec\": {\n",
    "            \"boot_disk_size_gb\": 1000,            \n",
    "        }\n",
    "    }\n",
    "\n",
    "worker_pool_specs_2 = {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": f\"g2-standard-{CPU_COUNT}\",\n",
    "            \"accelerator_type\": \"NVIDIA_L4\",\n",
    "            \"accelerator_count\": ACCELERATOR_COUNT,       \n",
    "        },\n",
    "        \"replica_count\": REPLICA_COUNT - 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": IMAGE_URI,\n",
    "            \"command\": [],\n",
    "            \"args\": [],\n",
    "            \"env\": ENVS,                \n",
    "        },\n",
    "        \"disk_spec\": {\n",
    "            \"boot_disk_size_gb\": 1000,            \n",
    "        }\n",
    "    }\n",
    "worker_pool_specs = []\n",
    "if REPLICA_COUNT == 1:\n",
    "   worker_pool_specs = [worker_pool_specs_1 ]\n",
    "elif REPLICA_COUNT > 1:\n",
    "   worker_pool_specs = [worker_pool_specs_1, worker_pool_specs_2]\n",
    "\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "JOB_NAME  = \"DeepSpeed Chat Torchrun Test \" + TIMESTAMP\n",
    "\n",
    "my_job = aiplatform.CustomJob(\n",
    "    display_name=JOB_NAME,    \n",
    "    worker_pool_specs=worker_pool_specs,\n",
    ")\n",
    "\n",
    "# Checking Service account that will launch the job\n",
    "!gcloud config get account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454bf780-ddc1-4795-ac37-2fcb7abedeaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects/703099487153/locations/us-central1/tensorboards/2069606338916253696\n"
     ]
    }
   ],
   "source": [
    "#####\n",
    "# Either create or reuse a tensorboard\n",
    "# tensorboard = aiplatform.Tensorboard.create(\n",
    "#    display_name=JOB_NAME,\n",
    "# )\n",
    "# \n",
    "tensorboard_name = \"2069606338916253696\"\n",
    "tensorboard = aiplatform.Tensorboard(tensorboard_name=tensorboard_name)\n",
    "# \n",
    "print(tensorboard.resource_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "160cf979",
   "metadata": {},
   "source": [
    "### Running the CustomJob\n",
    "\n",
    "Custom Service Account - https://cloud.google.com/vertex-ai/docs/general/custom-service-account. For custom service account, be sure to first grant the SA running this notebook the \"Service Account User\" role, otherwise you won't be able to launch the job with the custom service account.\n",
    "\n",
    "Tensorboard - https://cloud.google.com/vertex-ai/docs/experiments/tensorboard-training. Your training script must be configured to write TensorBoard logs to the Cloud Storage bucket, the location of which the Vertex AI Training Service will automatically make available through a predefined environment variable AIP_TENSORBOARD_LOG_DIR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6c5133-e0b0-427d-ae81-da5ea8ab5c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating CustomJob\n",
      "CustomJob created. Resource name: projects/703099487153/locations/us-central1/customJobs/7199000499658424320\n",
      "To use this CustomJob in another session:\n",
      "custom_job = aiplatform.CustomJob.get('projects/703099487153/locations/us-central1/customJobs/7199000499658424320')\n",
      "View Custom Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/7199000499658424320?project=703099487153\n",
      "View Tensorboard:\n",
      "https://us-central1.tensorboard.googleusercontent.com/experiment/projects+703099487153+locations+us-central1+tensorboards+2069606338916253696+experiments+7199000499658424320\n"
     ]
    }
   ],
   "source": [
    "my_job.submit(    \n",
    "    enable_web_access=True, # For debugging\n",
    "    # network=\"projects/{PROJECT_NUMBER}/global/networks/{PEER_NETWORK_NAME}\",\n",
    "    service_account=\"703099487153-compute@developer.gserviceaccount.com\",\n",
    "    tensorboard=tensorboard.resource_name,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea803db1",
   "metadata": {},
   "source": [
    "While the Deepspeed Chat team has auto-tuning on roadmap, if you encounter CUDA OOM right now their advice is:\n",
    "- Reduce `--per_device_*_batch_size`,\n",
    "- Increase `--zero_stage {0,1,2,3}` on multi-gpu setups,\n",
    "- Enable `--gradient_checkpointing` or `--only_optimize_lora`,\n",
    "- Increase `--gradient_accumulate_steps {#}`, higher number reduces communication of gradients between steps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c2003404",
   "metadata": {},
   "source": [
    "# Configurations backup\n",
    "\n",
    "Testing facebook/opt-125m and Dahoas/synthetic-instruct-gptj-pairwise with 2 1xT4@n1-standard-4:\n",
    "\n",
    "PER_DEVICE_BATCH_SIZE - 8 will utilize < half a T4's memory on each of the 2 nodes, 32 uses the memory 70+%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eee199-e5df-4e5e-8761-aa99169cfcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opt-125m backup\n",
    "{\"name\": \"MODEL_PATH\", \"value\": \"facebook/opt-125m\"},                        \n",
    "{\"name\": \"DATA_PATHS\", \"value\": \"Dahoas/synthetic-instruct-gptj-pairwise\"},                        \n",
    "{\"name\": \"DATA_SPLIT\", \"value\": \"10,40,50\"},\n",
    "{\"name\": \"ZERO_STAGE\", \"value\": \"3\"},\n",
    "{\"name\": \"PER_DEVICE_BATCH_SIZE\", \"value\": \"32\"},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575e9dbe-a59b-48ed-b284-42024c6adb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama2 backup\n",
    "{\"name\": \"MODEL_PATH\", \"value\": \"meta-llama/Llama-2-7b-hf\"},                        \n",
    "{\"name\": \"DATA_PATHS\", \"value\": \"Dahoas/rm-static Dahoas/full-hh-rlhf Dahoas/synthetic-instruct-gptj-pairwise yitingxie/rlhf-reward-datasets\"},                        \n",
    "{\"name\": \"DATA_SPLIT\", \"value\": \"2,4,4\"},\n",
    "{\"name\": \"ZERO_STAGE\", \"value\": \"3\"},\n",
    "{\"name\": \"PER_DEVICE_BATCH_SIZE\", \"value\": \"4\"}, "
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m110",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m110"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

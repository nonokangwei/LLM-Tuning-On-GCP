torchrun --nnodes=1 --node_rank=0 \
--nproc_per_node=2 \
main.py \
--sft_only_data_path "samsum" \
--model_name_or_path "meta-llama/Llama-2-7b-hf" \
--per_device_train_batch_size 4 \
--per_device_eval_batch_size 4 \
--max_seq_len 512 \
--learning_rate 9.65e-6 \
--weight_decay 0. \
--num_train_epochs 1  \
--gradient_accumulation_steps 1 \
--lr_scheduler_type cosine \
--num_warmup_steps 0 \
--seed 1234 \
--gradient_checkpointing \
--zero_stage 3 \
--deepspeed \
--output_dir output \
--print_loss

torchrun \
--nnodes=4 \
--node_rank=3 \
--master_addr=10.148.3.5 \
--master_port=1111 \
--nproc_per_node=2 \
main.py \
--data_path "samsum" \
--data_split 10,0,0 \
--model_name_or_path "meta-llama/Llama-2-7b-hf" \
--per_device_train_batch_size 4 \
--per_device_eval_batch_size 4 \
--max_seq_len 512 \
--learning_rate 9.65e-6 \
--weight_decay 0. \
--num_train_epochs 1  \
--gradient_accumulation_steps 1 \
--lr_scheduler_type cosine \
--num_warmup_steps 0 \
--seed 1234 \
--gradient_checkpointing \
--zero_stage 3 \
--deepspeed \
--output_dir output \
--print_loss

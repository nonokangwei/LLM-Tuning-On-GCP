{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2f41531-961c-44e3-9f1a-e203ac016bc0",
   "metadata": {},
   "source": [
    "# Llama2 end-to-end training and inference pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93a0d56-11a6-4415-856f-2f88eb4bebdb",
   "metadata": {},
   "source": [
    "This note book shows how to create and setup an end-to-end training and deployment and inference pipeline, to demo LLMOps on Vertex AI. The model is Llama2, training based on Deepspeed-Chat, and Torchrun launcher. Deployment is based on vLLM. The training and serving are on Vertex AI. The pipeline also leverages Firestore(DataStore) to store model and endpoint information for future retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline DAG is as below:\n",
    "\n",
    "<div align=center><img src=\"./pipeline-DAG.png\" alt= “” width=\"500\" height=\"320\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad46cc4-6c81-43fd-ac17-b10a5c3381f6",
   "metadata": {},
   "source": [
    "## Build serving docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca780444-685a-4588-b399-517ee2d0aeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd LLM-Tuning-On-GCP/Serve/vLLM-on-Vertex\n",
    "PROJECT_ID=\"YOUR_PROJECT_ID\"\n",
    "LOCATION=\"us-central1\"\n",
    "AR_REPO=\"llama2\"\n",
    "IMAGE_NAME=\"llama2-serving\"\n",
    "TAG=\"vllm\"\n",
    "SERVE_IMAGE_NAME=f\"{LOCATION}-docker.pkg.dev/{PROJECT_ID}/{AR_REPO}/{IMAGE_NAME}:{TAG}\"\n",
    "\n",
    "# You can use gcloud build to build the docker image directl in cloud\n",
    "! gcloud builds submit --region=us-central1 --tag  . $SERVE_IMAGE_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20af627d-2043-410a-ad51-910d8f01f7f7",
   "metadata": {},
   "source": [
    "## Build training docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa41ca84-ed01-40e1-9c7c-a2597f84287d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd LLM-Tuning-On-GCP/Train/Deepspeed/torchrun-launcher\n",
    "AR_REPO=\"llama2\"\n",
    "IMG_NAME=\"deepspeed-chat\"\n",
    "TAG=\"vertex-torchrun\"\n",
    "TRAIN_IMAGE_URI=f\"{LOCATION}-docker.pkg.dev/{PROJECT_ID}/{AR_REPO}/{IMG_NAME}:{TAG}\"\n",
    "EXAMPLE_DIR=\"deepspeed-chat\"\n",
    "DOCKERFILE=f\"examples/{EXAMPLE_DIR}/{IMG_NAME}.Dockerfile\"\n",
    "\n",
    "# You can also build docker image locally and push it artifact registry\n",
    "! gcloud auth configure-docker\n",
    "! docker build . -t $TRAIN_IMAGE_URI -f $DOCKERFILE\n",
    "! docker push $TRAIN_IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa89c83-4874-47c9-8fdf-af88b31b8ddb",
   "metadata": {},
   "source": [
    "## Install libary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4af2827-19cd-4865-9c77-b42d05911ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install google-cloud-aiplatform --upgrade -qq\n",
    "! pip3 install google-cloud-pipeline-components --upgrade -qq\n",
    "! pip3 install kfp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaa7d1f-f6da-4ad6-a8e2-781ca1652585",
   "metadata": {},
   "source": [
    "## Construct pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19505a10-9b18-465f-b187-9fb26ef9fae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate UUID for job name\n",
    "import random\n",
    "import string\n",
    "from datetime import datetime\n",
    "\n",
    "# Generate a uuid of a specifed length(default=8)\n",
    "def generate_uuid(length: int = 8) -> str:\n",
    "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "UUID = generate_uuid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2578903-84b0-4e1c-b6fb-382caf954551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import google.cloud.aiplatform as aiplatform\n",
    "import kfp\n",
    "from kfp import compiler, dsl\n",
    "from kfp.dsl import Artifact, Dataset, Input, Metrics, Model, Output, component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360bfac1-790c-4b36-a931-abd5d03bf9ef",
   "metadata": {},
   "source": [
    "### Train component using Vertex Custom Training\n",
    "\n",
    "1. This component only submits job, and not waiting for its status update\n",
    "2. Training job id is returned as output, and logging uri is in artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91d95d58-00bb-4724-8613-d91869d4f65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image='python:3.10',\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-aiplatform\",\n",
    "        \"google-cloud-datastore\"\n",
    "    ],\n",
    ")\n",
    "def train_llama2(\n",
    "      project: str,\n",
    "      location: str,\n",
    "      stage_bucket: str,\n",
    "      train_job_display_name: str,\n",
    "      machine_type: str,\n",
    "      accelerator_type: str,\n",
    "      train_container_image_uri: str,\n",
    "      llm_model_uri: str,\n",
    "      data_uri: str,\n",
    "      service_account: str,\n",
    "      tensorboard_name: str,\n",
    "      output_gcs_uri: str,\n",
    "      log_uri: Output[Artifact],\n",
    "      accelerator_count: int = 4,\n",
    "      replica_count: int = 2,\n",
    "      per_device_batch_size: int = 4,\n",
    "      \n",
    ") -> str:\n",
    "    from datetime import datetime\n",
    "    import google.cloud.aiplatform as aiplatform\n",
    "    aiplatform.init(project=project, location=location, staging_bucket=stage_bucket)\n",
    "    \n",
    "    ENVS = [\n",
    "        {\"name\": \"MODEL_PATH\", \"value\": llm_model_uri},                   \n",
    "        {\"name\": \"DATA_PATHS\", \"value\": data_uri},\n",
    "        {\"name\": \"DATA_SPLIT\", \"value\": \"10,0,0\"},\n",
    "        {\"name\": \"ZERO_STAGE\", \"value\": \"3\"},\n",
    "        {\"name\": \"PER_DEVICE_BATCH_SIZE\", \"value\": f\"{per_device_batch_size}\"},\n",
    "        {\"name\": \"NUM_GPU_PER_NODE\", \"value\": f\"{accelerator_count}\"},\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    worker_pool_specs_1 = {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": machine_type,\n",
    "            \"accelerator_type\": accelerator_type,\n",
    "            \"accelerator_count\": accelerator_count,       \n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": train_container_image_uri,\n",
    "            \"command\": [],\n",
    "            \"args\": [],\n",
    "            \"env\": ENVS,                \n",
    "        },\n",
    "        \"disk_spec\": {\n",
    "            \"boot_disk_size_gb\": 1000,            \n",
    "        }\n",
    "    }\n",
    "    worker_pool_specs_2 = {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": machine_type,\n",
    "            \"accelerator_type\": accelerator_type,\n",
    "            \"accelerator_count\": accelerator_count,       \n",
    "        },\n",
    "        \"replica_count\": replica_count - 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": train_container_image_uri,\n",
    "            \"command\": [],\n",
    "            \"args\": [],\n",
    "            \"env\": ENVS,                \n",
    "        },\n",
    "        \"disk_spec\": {\n",
    "            \"boot_disk_size_gb\": 1000,            \n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if replica_count == 1:\n",
    "        worker_pool_specs = [worker_pool_specs_1 ]\n",
    "    elif replica_count > 1:\n",
    "        worker_pool_specs = [worker_pool_specs_1, worker_pool_specs_2]\n",
    "    \n",
    "    #build training job and submit\n",
    "    tensorboard = aiplatform.Tensorboard(tensorboard_name=tensorboard_name)\n",
    "    my_job = aiplatform.CustomJob(\n",
    "        display_name=train_job_display_name,    \n",
    "        worker_pool_specs=worker_pool_specs,\n",
    "        base_output_dir=output_gcs_uri,)\n",
    "    my_job.submit(    \n",
    "        enable_web_access=True,\n",
    "        service_account=service_account,\n",
    "        tensorboard=tensorboard.resource_name,    \n",
    "    )\n",
    "    \n",
    "    # job id is output, and log uri is artifact\n",
    "    job_uri = my_job.to_dict()[\"name\"]\n",
    "    job_id = job_uri.split(\"/\")[-1]\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    log_uri_value = f\"https://pantheon.corp.google.com/logs/query;query=resource.labels.job_id%3D%22{job_id}%22%20timestamp%3E%3D%22{timestamp}%22;duration=PT3H?mods=-ai_platform_fake_service&project={project}\"\n",
    "    log_uri.metadata[\"train_log_uri\"] = log_uri_value\n",
    "    \n",
    "    #save to datastore\n",
    "    from google.cloud import datastore\n",
    "    datastore_client = datastore.Client(project=project)\n",
    "    kind = \"train\"\n",
    "    \n",
    "    name = \"train_job_uri\"\n",
    "    info_key = datastore_client.key(kind, name)\n",
    "    info = datastore_client.get(info_key)\n",
    "    if info == None:\n",
    "        info = datastore.Entity(key=info_key)\n",
    "        info[\"value\"] = job_uri\n",
    "        datastore_client.put(info)\n",
    "    else:\n",
    "        info[\"value\"] = job_uri\n",
    "        datastore_client.put(info)\n",
    "    \n",
    "    name = \"train_log_uri\"\n",
    "    info_key = datastore_client.key(kind, name)\n",
    "    info = datastore_client.get(info_key)\n",
    "    if info == None:\n",
    "        info = datastore.Entity(key=info_key)\n",
    "        info[\"value\"] = log_uri_value\n",
    "        datastore_client.put(info)\n",
    "    else:\n",
    "        info[\"value\"] = log_uri_value\n",
    "        datastore_client.put(info)\n",
    "        \n",
    "    return job_uri\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f60e9aa-4ca1-4d22-8656-cf35dc20dc82",
   "metadata": {},
   "source": [
    "### Get training status\n",
    "\n",
    "1. Looply check training job status\n",
    "2. If success, component successfully finishes\n",
    "3. If fail, we make the component failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88d144de-68e0-4984-bba1-0c6c53eb8c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image='python:3.10',\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-aiplatform\",\n",
    "    ],\n",
    ")\n",
    "def get_train_status(\n",
    "      project: str,\n",
    "      location: str,\n",
    "      stage_bucket: str,\n",
    "      job_name: str\n",
    "):\n",
    "    import google.cloud.aiplatform as aiplatform\n",
    "    import time\n",
    "    import logging\n",
    "    \n",
    "    aiplatform.init(project=project, location=location, staging_bucket=stage_bucket)\n",
    "    job_instance = aiplatform.CustomJob.get(resource_name=job_name)\n",
    "    logging.info(\"Start outputing training status here!\")\n",
    "    \n",
    "    #if success, component successfully finishes, if fail, component failed.\n",
    "    while(1):\n",
    "        logging.info(f\"{job_name}: {str(job_instance.state)}\")\n",
    "        assert str(job_instance.state) != \"JobState.JOB_STATE_FAILED\"\n",
    "        if str(job_instance.state) == \"JobState.JOB_STATE_SUCCEEDED\":\n",
    "            break\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4164d5-66ef-4bc4-af2c-005fb5ebacc5",
   "metadata": {},
   "source": [
    "### Upload model to model registry\n",
    "\n",
    "1. Check if model exists\n",
    "2. If model doesn't exist, create a new model\n",
    "3. If model exists, upload a new version and set the new version as default\n",
    "4. Save model id to datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09650a73-ab07-4d5b-b769-832cb3fdd6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image='python:3.10',\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-aiplatform\",\n",
    "        \"google-auth\",\n",
    "        \"google-api-core\",\n",
    "        \"google-cloud-datastore\"\n",
    "    ],\n",
    ")\n",
    "def upload_model(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    model_display_name: str,\n",
    "    model_uri: str,\n",
    "    serving_container_uri: str,\n",
    ") -> str: \n",
    "    import json\n",
    "    import os\n",
    "    import sys\n",
    "    import logging\n",
    "    import requests\n",
    "    import time\n",
    "    from datetime import datetime\n",
    "\n",
    "    from google.cloud import aiplatform\n",
    "    from google.auth.transport.requests import Request\n",
    "    import google.auth\n",
    "    from google.api_core import operations_v1\n",
    "    \n",
    "    \n",
    "    model_name = f\"projects/{project}/locations/{location}/models/{model_display_name}\"\n",
    "    model_if_exist = 1\n",
    "    try:\n",
    "        model_instance = aiplatform.Model(model_name=model_name)\n",
    "    except:\n",
    "        model_if_exist = 0\n",
    " \n",
    "    \n",
    "       \n",
    "    # start upload model\n",
    "    model_endpoint_url = f\"https://{location}-aiplatform.googleapis.com/v1/projects/{project}/locations/{location}/models:upload\"\n",
    "    \n",
    "    # if you have lora model, use model_gcs_uri and peft_model_gcs_uri in args\n",
    "    # --model_gcs_uri=gcs_uri\n",
    "    # --peft_model_gcs_uri=gcs_uri\n",
    "    # remove --model\n",
    "    model_container_spec = {\n",
    "      \"imageUri\": serving_container_uri,\n",
    "      \"command\": [\n",
    "          \"python3\",\n",
    "          \"/root/scripts/launcher.py\"\n",
    "        ],\n",
    "      \"args\": [\n",
    "          \"--host=0.0.0.0\",\n",
    "          \"--port=7080\",\n",
    "          f\"--model={model_uri}/model\",\n",
    "          \"--tensor-parallel-size=2\",\n",
    "          \"--swap-space=16\"\n",
    "      ],\n",
    "      \"ports\": [\n",
    "        {\n",
    "          \"containerPort\": 7080\n",
    "        }\n",
    "      ],\n",
    "      \"predictRoute\": \"/generate\",\n",
    "      \"healthRoute\": \"/ping\",\n",
    "      \"sharedMemorySizeMb\": \"6000\"\n",
    "    }\n",
    "\n",
    "    model_source_info = {\n",
    "        \"sourceType\": \"CUSTOM\"\n",
    "    }\n",
    "\n",
    "    # if model doesn't exist, creat a new model, if model exists, upload a new version and set it as default version\n",
    "    if model_if_exist == 0:\n",
    "        model_info = {\n",
    "          \"displayName\": f\"{model_display_name}\",\n",
    "          \"containerSpec\": model_container_spec,\n",
    "          \"modelSourceInfo\": model_source_info\n",
    "        }\n",
    "            \n",
    "        model_request = {\n",
    "          \"modelId\": f\"{model_display_name}\",\n",
    "          \"model\": model_info\n",
    "        }\n",
    "    else:\n",
    "        model_info = {\n",
    "          \"displayName\": f\"{model_display_name}\",\n",
    "          \"containerSpec\": model_container_spec,\n",
    "          \"modelSourceInfo\": model_source_info,\n",
    "          \"versionAliases\": \"default\"\n",
    "        }\n",
    "        \n",
    "        model_request = {\n",
    "          \"parentModel\": f\"{model_name}\",\n",
    "          \"model\": model_info\n",
    "        }\n",
    "\n",
    "    # Get the default credentials\n",
    "    credentials, _ = google.auth.default()\n",
    "\n",
    "    # Request an access token\n",
    "    credentials.refresh(Request())\n",
    "\n",
    "    # Get the access token\n",
    "    access_token = credentials.token\n",
    "\n",
    "    # Init Model upload request header\n",
    "    headers = {\n",
    "            \"Authorization\": f\"Bearer {access_token}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "\n",
    "    response = requests.post(model_endpoint_url, headers=headers, data=json.dumps(model_request))\n",
    "    \n",
    "    # parse the JSON response\n",
    "    data = response.json()\n",
    "    print(data)\n",
    "    logging.info(data)\n",
    "    # extract the operation ID\n",
    "    operation_name = data['name']\n",
    "    \n",
    "    request = google.auth.transport.requests.Request()\n",
    "    channel = google.auth.transport.grpc.secure_authorized_channel(\n",
    "            credentials, request, f\"{location}-aiplatform.googleapis.com\")\n",
    "    \n",
    "    # create an operations client\n",
    "    client = operations_v1.OperationsClient(channel=channel)\n",
    "\n",
    "    # check if the operation is done\n",
    "    while(True):\n",
    "        # wait for 30 secs\n",
    "        time.sleep(30)\n",
    "        \n",
    "        # get the operation\n",
    "        operation = client.get_operation(operation_name)\n",
    "        \n",
    "        if operation.done:\n",
    "            if operation.HasField('response'):\n",
    "                print('Operation completed successfully')\n",
    "                # you can access the response via operation.response\n",
    "                break\n",
    "            elif operation.HasField('error'):\n",
    "                print('Operation failed')\n",
    "                # you can access the error message via operation.error.message\n",
    "                raise Exception(f\"This is error when upload model: {operation.error.message}\")\n",
    "        else:\n",
    "            logging.info('Operation still in progress')\n",
    "    \n",
    "    # Define your model name\n",
    "    model_name = f\"projects/{project}/locations/{location}/models/{model_display_name}\"\n",
    "    \n",
    "    # save to datastore\n",
    "    from google.cloud import datastore\n",
    "    datastore_client = datastore.Client(project=project)\n",
    "    kind = \"model\"\n",
    "    name = model_display_name\n",
    "    info_key = datastore_client.key(kind, name)\n",
    "    info = datastore_client.get(info_key) \n",
    "    if info == None:\n",
    "        info = datastore.Entity(key=info_key)\n",
    "        info[\"value\"] = model_name\n",
    "        datastore_client.put(info)\n",
    "    else:\n",
    "        info[\"value\"] = model_name\n",
    "        datastore_client.put(info)\n",
    "\n",
    "    \n",
    "    return model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831505d1-71ad-4bee-8f30-26d49cb1222c",
   "metadata": {},
   "source": [
    "### Create endpoint\n",
    "\n",
    "1. Get endpoint to check if it exists\n",
    "2. If exists, it will skip endpoint creation\n",
    "3. If doesn't exist, endpoint will be crerated\n",
    "3. Save endpoint id to datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a71de0c1-73ba-494a-a857-e2df1157a87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image='python:3.10',\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-aiplatform\",\n",
    "        \"google-cloud-datastore\",\n",
    "        \"google-auth\",\n",
    "        \"google-api-core\"\n",
    "    ],\n",
    ")\n",
    "def create_endpoint(\n",
    "    endpoint_display_name: str,\n",
    "    project: str,\n",
    "    location: str,\n",
    "    stage_bucket: str\n",
    ") -> str:\n",
    "    \n",
    "    import json\n",
    "    import os\n",
    "    import sys\n",
    "    import logging\n",
    "    import requests\n",
    "    import time\n",
    "    from datetime import datetime\n",
    "\n",
    "    from google.cloud import aiplatform\n",
    "    from google.auth.transport.requests import Request\n",
    "    import google.auth\n",
    "    from google.api_core import operations_v1\n",
    "    \n",
    "    \n",
    "    model_endpoint_url = f\"https://{location}-aiplatform.googleapis.com/v1/projects/{project}/locations/{location}/endpoints?filter=display_name={endpoint_display_name}\"\n",
    "    # Get the default credentials\n",
    "    credentials, _ = google.auth.default()\n",
    "\n",
    "    # Request an access token\n",
    "    credentials.refresh(Request())\n",
    "\n",
    "    # Get the access token\n",
    "    access_token = credentials.token\n",
    "\n",
    "    # Init Model upload request header\n",
    "    headers = {\n",
    "            \"Authorization\": f\"Bearer {access_token}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "\n",
    "    response = requests.get(model_endpoint_url, headers=headers)\n",
    "    \n",
    "    # parse the JSON response\n",
    "    data = response.json()\n",
    "    logging.info(data)\n",
    "   \n",
    "    # if endpoint exists, get its id, if it doesn't exist, create endpoint first\n",
    "    endpoint_id = None\n",
    "    if data != {}:\n",
    "        endpoint_id = data['endpoints'][0]['name']\n",
    "        \n",
    "    if endpoint_id == None:\n",
    "        from google.cloud import aiplatform\n",
    "        aiplatform.init(project=project, location=location, staging_bucket=stage_bucket)\n",
    "        \"\"\"Deploys trained models with vLLM into Vertex AI.\"\"\"\n",
    "        endpoint = aiplatform.Endpoint.create(display_name=endpoint_display_name)\n",
    "        endpoint_id = endpoint.resource_name\n",
    "    \n",
    "    # save to datastore\n",
    "    from google.cloud import datastore\n",
    "    datastore_client = datastore.Client(project=project)\n",
    "    kind = \"endpoint\"\n",
    "    info_key = datastore_client.key(kind, endpoint_display_name)\n",
    "    info = datastore_client.get(info_key) \n",
    "    if info == None:\n",
    "        info = datastore.Entity(key=info_key)\n",
    "        info[\"value\"] = endpoint_id\n",
    "        datastore_client.put(info)\n",
    "    else:\n",
    "        info[\"value\"] = endpoint_id\n",
    "        datastore_client.put(info)\n",
    "\n",
    "    return endpoint_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662017a1-d000-44c9-be98-c3beb7b36996",
   "metadata": {},
   "source": [
    "### Deploy model to endpoint\n",
    "\n",
    "1. Check if model has been deployed to endpoint\n",
    "2. If deployed, undeploy the model if the tag \"if_force_undeploy\" set as True, undeploy model first and then deploy the new model\n",
    "3. If deployed, if the tag \"if_force_undeploy\" set as False, skip deploying model step and return directly\n",
    "4. If not deployed, deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d170b402-5d9f-4130-a8fa-a824f964d73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image='python:3.10',\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-aiplatform\",\n",
    "        \"google-cloud-datastore\",\n",
    "        \"google-auth\",\n",
    "        \"google-api-core\"\n",
    "    ],\n",
    ")\n",
    "def deploy_model_vllm(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    stage_bucket: str,\n",
    "    service_account: str,\n",
    "    model_name: str,\n",
    "    endpoint_name: str,\n",
    "    if_force_undeploy: bool = True,\n",
    "    machine_type: str = \"n1-standard-8\",\n",
    "    accelerator_type: str = \"NVIDIA_TESLA_V100\",\n",
    "    accelerator_count: int = 1,\n",
    ") -> str:\n",
    "    \n",
    "    import json\n",
    "    import os\n",
    "    import sys\n",
    "    import logging\n",
    "    import requests\n",
    "    import time\n",
    "    from datetime import datetime\n",
    "\n",
    "    from google.cloud import aiplatform\n",
    "    from google.auth.transport.requests import Request\n",
    "    import google.auth\n",
    "    from google.api_core import operations_v1\n",
    "    \n",
    "    # get endpoint to check if any models have been deployed to it\n",
    "    model_endpoint_url = f\"https://{location}-aiplatform.googleapis.com/v1/{endpoint_name}\"\n",
    "    # Get the default credentials\n",
    "    credentials, _ = google.auth.default()\n",
    "\n",
    "    # Request an access token\n",
    "    credentials.refresh(Request())\n",
    "\n",
    "    # Get the access token\n",
    "    access_token = credentials.token\n",
    "\n",
    "    # Init Model upload request header\n",
    "    headers = {\n",
    "            \"Authorization\": f\"Bearer {access_token}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "\n",
    "    response = requests.get(model_endpoint_url, headers=headers)\n",
    "    \n",
    "    # parse the JSON response\n",
    "    data = response.json()\n",
    "    logging.info(data)\n",
    "    deployed = 1\n",
    "    try:\n",
    "        data['deployedModels']\n",
    "    except:\n",
    "        deployed = 0 \n",
    "    \n",
    "    \n",
    "    from google.cloud import aiplatform\n",
    "    aiplatform.init(project=project, location=location, staging_bucket=stage_bucket)\n",
    "    \n",
    "    model_instance = aiplatform.Model(model_name=model_name)\n",
    "    endpoint_instance = aiplatform.Endpoint(endpoint_name=endpoint_name)\n",
    "    status = \"Deployed\"\n",
    "    if deployed == 1:\n",
    "        if if_force_undeploy == False:\n",
    "            status = \"Existed and ignore deployment\"\n",
    "            return status\n",
    "        else:\n",
    "            endpoint_instance.undeploy_all(sync=True)\n",
    "            status = \"Existed, undeploy and redeploy\"\n",
    "\n",
    "    model_instance.deploy(\n",
    "        endpoint=endpoint_instance,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "    )\n",
    "    return status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c13767-d923-414c-bccf-7fe3b27e6f7a",
   "metadata": {},
   "source": [
    "### Construct the pipeline based on all the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0ac367bf-885c-45cc-b133-7c9fe001589b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(name=\"train-endpoint-deploy\" + UUID)\n",
    "def pipeline(\n",
    "      project: str,\n",
    "      location: str,\n",
    "      stage_bucket: str,\n",
    "      train_job_display_name: str,\n",
    "      train_machine_type: str,\n",
    "      train_accelerator_type: str,\n",
    "      train_accelerator_count: int,\n",
    "      replica_count: int,\n",
    "      train_container_image_uri: str,\n",
    "      llm_model_uri: str,\n",
    "      data_uri: str,\n",
    "      per_device_batch_size: int,\n",
    "      output_gcs_uri: str,\n",
    "      service_account: str,\n",
    "      tensorboard: str,\n",
    "      model_display_name: str,\n",
    "      endpoint_display_name: str,\n",
    "      if_force_undeploy: bool,\n",
    "      serving_container_image_uri: str,\n",
    "      serving_machine_type: str,\n",
    "      serving_accelerator_type: str,\n",
    "      serving_accelerator_count: int,\n",
    "):\n",
    "\n",
    "    train_op = train_llama2(\n",
    "      project=project,\n",
    "      location=location,\n",
    "      stage_bucket=stage_bucket,\n",
    "      train_job_display_name=train_job_display_name,\n",
    "      machine_type=train_machine_type,\n",
    "      accelerator_type=train_accelerator_type,\n",
    "      replica_count=replica_count,\n",
    "      accelerator_count=train_accelerator_count,\n",
    "      train_container_image_uri=train_container_image_uri,\n",
    "      llm_model_uri=llm_model_uri,\n",
    "      data_uri=data_uri,\n",
    "      per_device_batch_size=per_device_batch_size,\n",
    "      output_gcs_uri=output_gcs_uri,\n",
    "      service_account=service_account,\n",
    "      tensorboard_name=tensorboard,\n",
    "    )\n",
    "    \n",
    "    status_op = get_train_status(\n",
    "      project=project,\n",
    "      location=location,\n",
    "      stage_bucket=stage_bucket,\n",
    "      job_name=train_op.outputs[\"Output\"]\n",
    "    ).after(train_op)\n",
    "    \n",
    "    upload_model_op = upload_model(\n",
    "        project=project,\n",
    "        location=location,\n",
    "        model_display_name=model_display_name,\n",
    "        model_uri=output_gcs_uri,\n",
    "        serving_container_uri=serving_container_image_uri,\n",
    "    ).after(status_op)\n",
    "    \n",
    "    create_endpoint_op = create_endpoint(\n",
    "        endpoint_display_name=endpoint_display_name,\n",
    "        project=project,\n",
    "        location=location,\n",
    "        stage_bucket=stage_bucket,\n",
    "    )\n",
    "    \n",
    "    deploy_op = deploy_model_vllm(\n",
    "        project=project,\n",
    "        location=location,\n",
    "        stage_bucket=stage_bucket,\n",
    "        service_account=service_account,\n",
    "        model_name=upload_model_op.output,\n",
    "        endpoint_name=create_endpoint_op.output,\n",
    "        if_force_undeploy=if_force_undeploy,\n",
    "        machine_type=serving_machine_type,\n",
    "        accelerator_type=serving_accelerator_type,\n",
    "        accelerator_count=serving_accelerator_count,\n",
    "    ).after(upload_model_op)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec73853b-79c9-4eab-87eb-e2a01f49e1e5",
   "metadata": {},
   "source": [
    "### Compile the pipeline template and save to local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7eb930f4-8230-4de4-8136-e4a9d3ea7266",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import compiler \n",
    "package_path = \"llama2_deepspeed_pipeline.json\"\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path=package_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e29f60-0d08-4ee0-9481-5ddb6156d7eb",
   "metadata": {},
   "source": [
    "## Config parameters and submit a pipeline job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7b3939ef-18ee-42ee-96b6-fec789efdfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create working dir to pass to job spec\n",
    "PROJECT_ID = \"YOUR-PROJECT-ID\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"  # @param {type: \"string\"}\n",
    "BUCKET_URI = \"gs://BUCKET-NAME/pipeline\"  # @param {type:\"string\"}\n",
    "PIPELINE_ROOT = f\"{BUCKET_URI}/pipeline_root/llama2\"\n",
    "TENSORBOARD_NAME = \"TENSORBOARD-ID\" # @param {type:\"string\"}\n",
    "SERVICE_ACCOUNT = \"SERVICE-ACCOUNT\"  # @param {type:\"string\"}\n",
    "WORKING_DIR = f\"{PIPELINE_ROOT}/{UUID}\"\n",
    "\n",
    "train_job_display_name = f'llama2-custom-train-{datetime.today().strftime(\"%Y%m%d%M%S\")}' # @param {type:\"string\"}\n",
    "TRAIN_IMAGE_URI = TRAIN_IMAGE_URI\n",
    "llm_model_uri = \"/gcs/deepspeed_repo/base_model/Llama-2-7b-hf/Llama-2-7b-hf\" # @param {type:\"string\"}\n",
    "data_uri = \"/gcs/deepspeed_repo/dataset/samsum\" # @param {type:\"string\"}\n",
    "\n",
    "TRAIN_ACCELERATOR_TYPE = \"NVIDIA_L4\"\n",
    "TOTAL_GPU_NEEDED = 8\n",
    "TRAIN_ACCELERATOR_COUNT = 4\n",
    "CPU_UNIT = 12\n",
    "CPU_COUNT = CPU_UNIT * TRAIN_ACCELERATOR_COUNT\n",
    "TRAIN_MACHINE_TYPE = f\"g2-standard-{CPU_COUNT}\"\n",
    "TRAIN_REPLICA_COUNT = int(TOTAL_GPU_NEEDED / TRAIN_ACCELERATOR_COUNT)\n",
    "\n",
    "MODEL_DISPLAY_NAME = f\"llama2_pipeline_vllm_{UUID}\"\n",
    "ENDPOINT_DISPLAY_NAME = f\"{MODEL_DISPLAY_NAME}-endpoint\"\n",
    "IF_FORCE_UNDEPLOY = True\n",
    "SERVING_IMAGE_URI = SERVE_IMAGE_URI\n",
    "SERVING_MACHINE_TYPE=\"g2-standard-24\"\n",
    "SERVING_ACCELERATOR_TYPE=\"NVIDIA_L4\"\n",
    "SERVING_ACCELERATOR_COUNT=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c8b13edc-f630-4c07-8c29-ebc1a9a6b31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "615a0c0b-3419-4dfa-9719-f3054b02fde2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/703099487153/locations/us-central1/tensorboards/2069606338916253696'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tensorboard = aiplatform.Tensorboard(tensorboard_name=TENSORBOARD_NAME)\n",
    "#tensorboard.resource_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1ced61-6b4c-4a76-8cf3-45e08adfdab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/703099487153/locations/us-central1/pipelineJobs/train-endpoint-deploy0qehgs0f-20231201072301\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/703099487153/locations/us-central1/pipelineJobs/train-endpoint-deploy0qehgs0f-20231201072301')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/train-endpoint-deploy0qehgs0f-20231201072301?project=703099487153\n",
      "PipelineJob projects/703099487153/locations/us-central1/pipelineJobs/train-endpoint-deploy0qehgs0f-20231201072301 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/703099487153/locations/us-central1/pipelineJobs/train-endpoint-deploy0qehgs0f-20231201072301 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/703099487153/locations/us-central1/pipelineJobs/train-endpoint-deploy0qehgs0f-20231201072301 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/703099487153/locations/us-central1/pipelineJobs/train-endpoint-deploy0qehgs0f-20231201072301 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/703099487153/locations/us-central1/pipelineJobs/train-endpoint-deploy0qehgs0f-20231201072301 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/703099487153/locations/us-central1/pipelineJobs/train-endpoint-deploy0qehgs0f-20231201072301 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/703099487153/locations/us-central1/pipelineJobs/train-endpoint-deploy0qehgs0f-20231201072301 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/703099487153/locations/us-central1/pipelineJobs/train-endpoint-deploy0qehgs0f-20231201072301 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/703099487153/locations/us-central1/pipelineJobs/train-endpoint-deploy0qehgs0f-20231201072301 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "DISPLAY_NAME = \"llama2_deepspeed_pipeline_\" + UUID\n",
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    template_path=package_path,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values= {\n",
    "    'train_job_display_name': train_job_display_name,\n",
    "    'project': PROJECT_ID,\n",
    "    'location': REGION,\n",
    "    'stage_bucket': BUCKET_URI,\n",
    "    'train_machine_type': TRAIN_MACHINE_TYPE,\n",
    "    'train_accelerator_type': TRAIN_ACCELERATOR_TYPE,\n",
    "    'train_accelerator_count': TRAIN_ACCELERATOR_COUNT,\n",
    "    'replica_count': TRAIN_REPLICA_COUNT,\n",
    "    'train_container_image_uri': TRAIN_IMAGE_URI,\n",
    "    'llm_model_uri': llm_model_uri,\n",
    "    'data_uri': data_uri,\n",
    "    'per_device_batch_size': 4,\n",
    "    'output_gcs_uri': WORKING_DIR,\n",
    "    'service_account': SERVICE_ACCOUNT,\n",
    "    'tensorboard': TENSORBOARD_NAME,\n",
    "    'model_display_name': MODEL_DISPLAY_NAME,\n",
    "    'endpoint_display_name': ENDPOINT_DISPLAY_NAME,\n",
    "    'if_force_undeploy': IF_FORCE_UNDEPLOY,\n",
    "    'serving_container_image_uri': SERVING_IMAGE_URI,\n",
    "    'serving_machine_type': SERVING_MACHINE_TYPE,\n",
    "    'serving_accelerator_type': SERVING_ACCELERATOR_TYPE,\n",
    "    'serving_accelerator_count': SERVING_ACCELERATOR_COUNT,\n",
    "      \n",
    "  },\n",
    "    enable_caching=True,\n",
    ")\n",
    "\n",
    "job.run(service_account=SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551b3ecd-eb31-4af4-949b-8321d0dcbc4d",
   "metadata": {},
   "source": [
    "## Inference test\n",
    "\n",
    "Get endpoint id from DataStore and test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c4fb55-0e2d-4aad-a677-d755257b7fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import datastore\n",
    "datastore_client = datastore.Client(project=project)\n",
    "kind = \"endpoint\"\n",
    "info_key = datastore_client.key(kind, ENDPOINT_DISPLAY_NAME)\n",
    "info = datastore_client.get(info_key)\n",
    "\n",
    "instance = {\n",
    "    \"prompt\": \"Hi, Google.\",\n",
    "    \"n\": 1,\n",
    "    \"max_tokens\": 50,\n",
    "    \"temperature\": 1.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"top_k\": 10,\n",
    "}\n",
    "endpoint_instance = aiplatform.Endpoint(endpoint_name=info[\"value\"])\n",
    "response = endpoint_instance.predict(instances=[instance])\n",
    "print(response.predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b81f475-79d3-4309-8bb5-2c59be1b58a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m110",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m110"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
